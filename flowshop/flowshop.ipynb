{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Sakura-RaidenMEI/Funsearch_on_flowshop/blob/main/flowshop/flowshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "4d295f7a4d43b2c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run FunSearch on Flowshop\n",
    "Five steps:\n",
    "1. Implement 'LLM' interface.\n",
    "2. Implement a 'SandBox' interface.\n",
    "3. Prepare a 'specification'.\n",
    "4. Prepare a dataset.\n",
    "5. Start FunSearch."
   ],
   "metadata": {
    "collapsed": false,
    "id": "58ba1915fced4e72"
   },
   "id": "58ba1915fced4e72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation: download the project file from github. And update system path."
   ],
   "metadata": {
    "collapsed": false,
    "id": "6a2d02b8e9c3ba67"
   },
   "id": "6a2d02b8e9c3ba67"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'Funsearch_on_flowshop' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Sakura-RaidenMEI/Funsearch_on_flowshop.git\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/content/Funsearch_on_flowshop/')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22453e8153e0934c",
    "outputId": "c45c666c-989e-4e68-9220-1aa68b16ae3d"
   },
   "id": "22453e8153e0934c"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "ZL3i4JLo2oDW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eda2c1d3-440a-48ba-8d04-a67d12c60131"
   },
   "id": "ZL3i4JLo2oDW",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Implement LLM interface\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fe47175708cc0a93"
   },
   "id": "fe47175708cc0a93"
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import json\n",
    "import multiprocessing\n",
    "from typing import Collection, Any\n",
    "import http.client\n",
    "from implementation import sampler\n",
    "\n",
    "\n",
    "def _trim_preface_of_body(sample: str) -> str:\n",
    "    \"\"\"Trim the redundant descriptions/symbols/'def' declaration before the function body.\n",
    "    Please see my comments in sampler.LLM (in sampler.py).\n",
    "    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.\n",
    "\n",
    "    -Example sample (function & description generated by LLM):\n",
    "    -------------------------------------\n",
    "    This is the optimized function ...\n",
    "    def priority_v2(...) -> ...:\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    -This function removes the description above the function's signature, and the function's signature.\n",
    "    -The indent of the code is preserved.\n",
    "    -Return of this function:\n",
    "    -------------------------------------\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    \"\"\"\n",
    "    lines = sample.splitlines()\n",
    "    func_body_lineno = 0\n",
    "    find_def_declaration = False\n",
    "    for lineno, line in enumerate(lines):\n",
    "        # find the first 'def' statement in the given code\n",
    "        if line[:3] == 'def':\n",
    "            func_body_lineno = lineno\n",
    "            find_def_declaration = True\n",
    "            break\n",
    "    if find_def_declaration:\n",
    "        code = ''\n",
    "        for line in lines[func_body_lineno + 1:]:\n",
    "            code += line + '\\n'\n",
    "        return code\n",
    "    return sample\n",
    "\n",
    "\n",
    "class LLMAPI(sampler.LLM):\n",
    "    \"\"\"Language model that predicts continuation of provided source code.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samples_per_prompt: int, trim=True):\n",
    "        super().__init__(samples_per_prompt)\n",
    "        additional_prompt = (\"Improve the scheduling heuristic to minimize makespan. \"\n",
    "                             \"You can change how jobs are ordered or inserted. \"\n",
    "                             \"Be creative. Think beyond NEH logic. \"\n",
    "                             \"Pls only generate neh_heuristic(processing_times: np.ndarray) function\"\n",
    "                             \"Use loops, conditionals, or clustering ideas. Only return valid Python code.\")\n",
    "\n",
    "        self._additional_prompt = additional_prompt\n",
    "        self._trim = trim\n",
    "\n",
    "    def draw_samples(self, prompt: str) -> Collection[str]:\n",
    "        \"\"\"Returns multiple predicted continuations of `prompt`.\"\"\"\n",
    "        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]\n",
    "\n",
    "    def _draw_sample(self, content: str) -> str:\n",
    "        prompt = '\\n'.join([content, self._additional_prompt])\n",
    "        message = [{'role': 'user', 'content': prompt}]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                conn = http.client.HTTPSConnection(\"api.bltcy.ai\")\n",
    "                payload = json.dumps({\n",
    "                    \"max_tokens\": 1024,\n",
    "                    \"model\": \"chatgpt-4o-latest\",\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                headers = {\n",
    "                    'Authorization': 'Bearer sk-OmRJlpj2aI4A3GLvA4Bd841fCfB04b3e9eF6D0D9984f1719',\n",
    "                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
    "                    'Content-Type': 'application/json'\n",
    "                }\n",
    "                conn.request(\"POST\", \"/v1/chat/completions\", payload, headers)\n",
    "                res = conn.getresponse()\n",
    "                data = res.read().decode(\"utf-8\")\n",
    "                data = json.loads(data)\n",
    "                response = data['choices'][0]['message']['content']\n",
    "                if self._trim:\n",
    "                    response = _trim_preface_of_body(response)\n",
    "                return response\n",
    "            except Exception:\n",
    "                time.sleep(2)\n",
    "                continue"
   ],
   "metadata": {
    "id": "1999e45c9a568b08",
    "ExecuteTime": {
     "end_time": "2025-04-06T07:37:44.257111Z",
     "start_time": "2025-04-06T07:37:44.180248Z"
    }
   },
   "id": "1999e45c9a568b08",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implement a 'SandBox' interface"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d27817cdec2cedfc"
   },
   "id": "d27817cdec2cedfc"
  },
  {
   "cell_type": "code",
   "source": [
    "from implementation import evaluator\n",
    "from implementation import evaluator_accelerate\n",
    "\n",
    "\n",
    "class Sandbox(evaluator.Sandbox):\n",
    "    \"\"\"Sandbox for executing generated code. Implemented by RZ.\n",
    "\n",
    "    RZ: Sandbox returns the 'score' of the program and:\n",
    "    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).\n",
    "    2) stops the execution of the code in time (avoid endless loop).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False, numba_accelerate=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            verbose         : Print evaluate information.\n",
    "            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions\n",
    "                              support numba acceleration, such as np.piecewise().\n",
    "        \"\"\"\n",
    "        self._verbose = verbose\n",
    "        self._numba_accelerate = False\n",
    "\n",
    "    def run(\n",
    "            self,\n",
    "            program: str,\n",
    "            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')\n",
    "            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.\n",
    "            inputs: Any,  # refers to the dataset\n",
    "            test_input: str,  # refers to the current instance\n",
    "            timeout_seconds: int,\n",
    "            **kwargs  # RZ: add this\n",
    "    ) -> tuple[Any, bool]:\n",
    "        \"\"\"Returns `function_to_run(test_input)` and whether execution succeeded.\n",
    "\n",
    "        RZ: If the generated code (generated by LLM) is executed successfully,\n",
    "        the output of this function is the score of a given program.\n",
    "        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.\n",
    "        \"\"\"\n",
    "        dataset = {test_input: inputs[test_input]}\n",
    "        try:\n",
    "            result_queue = multiprocessing.Queue()\n",
    "            process = multiprocessing.Process(\n",
    "                target=self._compile_and_run_function,\n",
    "                args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)\n",
    "            )\n",
    "            process.start()\n",
    "            process.join(timeout=timeout_seconds)\n",
    "            if process.is_alive():\n",
    "                # if the process is not finished in time, we consider the program illegal\n",
    "                process.terminate()\n",
    "                process.join()\n",
    "                results = None, False\n",
    "            else:\n",
    "                if not result_queue.empty():\n",
    "                    results = result_queue.get_nowait()\n",
    "                else:\n",
    "                    results = None, False\n",
    "\n",
    "            return results\n",
    "        except:\n",
    "            return None, False\n",
    "\n",
    "    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,\n",
    "                                  result_queue):\n",
    "        try:\n",
    "            # optimize the code (decorate function_to_run with @numba.jit())\n",
    "            if numba_accelerate:\n",
    "                program = evaluator_accelerate.add_numba_decorator(\n",
    "                    program=program,\n",
    "                    function_to_evolve=function_to_evolve\n",
    "                )\n",
    "            # compile the program, and maps the global func/var/class name to its address\n",
    "            all_globals_namespace = {}\n",
    "            # execute the program, map func/var/class to global namespace\n",
    "            exec(program, all_globals_namespace)\n",
    "            # get the pointer of 'function_to_run'\n",
    "            function_to_run = all_globals_namespace[function_to_run]\n",
    "            # return the execution results\n",
    "            results = function_to_run(dataset)\n",
    "            # the results must be int or float\n",
    "            if not isinstance(results, (int, float)):\n",
    "                result_queue.put((None, False))\n",
    "                return\n",
    "            result_queue.put((results, True))\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_msg = traceback.format_exc()\n",
    "            print(f\"[Sandbox Error] {error_msg}\")  # 控制台\n",
    "            # if raise any exception, we assume the execution failed\n",
    "            result_queue.put((None, False))"
   ],
   "metadata": {
    "id": "3e3d88a87535b6b2",
    "ExecuteTime": {
     "end_time": "2025-04-06T07:37:47.036264Z",
     "start_time": "2025-04-06T07:37:47.028759Z"
    }
   },
   "id": "3e3d88a87535b6b2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Prepare a 'specification'"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ec3a05827354f9ae"
   },
   "id": "ec3a05827354f9ae"
  },
  {
   "cell_type": "code",
   "source": [
    "specification = r'''\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_makespan(schedule: list[int], processing_times: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Compute the makespan (total completion time) for a given job schedule in a PFSP.\n",
    "    - schedule: list of job indices in the order they are processed.\n",
    "    - processing_times: 2D numpy array of shape (num_jobs, num_machines) with processing times for each job on each machine.\n",
    "    Returns the makespan (int) for the given order.\n",
    "    \"\"\"\n",
    "    num_jobs = len(schedule)\n",
    "    num_machines = processing_times.shape[1]\n",
    "    if num_jobs == 0:\n",
    "        return 0\n",
    "\n",
    "    completion_times = np.zeros((num_jobs, num_machines), dtype=int)\n",
    "    first_job = schedule[0]\n",
    "    completion_times[0, 0] = processing_times[first_job, 0]\n",
    "    for m in range(1, num_machines):\n",
    "        completion_times[0, m] = completion_times[0, m-1] + processing_times[first_job, m]\n",
    "\n",
    "    for i in range(1, num_jobs):\n",
    "        job = schedule[i]\n",
    "        completion_times[i, 0] = completion_times[i-1, 0] + processing_times[job, 0]\n",
    "        for m in range(1, num_machines):\n",
    "            completion_times[i, m] = max(completion_times[i, m-1], completion_times[i-1, m]) + processing_times[job, m]\n",
    "\n",
    "    return int(completion_times[-1, -1])\n",
    "\n",
    "\n",
    "@funsearch.run\n",
    "def evaluate(instances: dict) -> float:\n",
    "    \"\"\"\n",
    "    FunSearch evaluation function that computes the average makespan across multiple datasets.\n",
    "    - instances: dict mapping instance names to 2D numpy arrays (processing time matrices).\n",
    "    Returns the negative mean makespan (float) for optimization.\n",
    "    \"\"\"\n",
    "    makespans = []\n",
    "    for name in instances:\n",
    "        processing_times = instances[name]\n",
    "        if not isinstance(processing_times, np.ndarray):\n",
    "            print(f\"[ERROR] Instance {name} is not ndarray\")\n",
    "            continue\n",
    "        if not np.issubdtype(processing_times.dtype, np.integer):\n",
    "            processing_times = processing_times.astype(int)\n",
    "\n",
    "        schedule = neh_heuristic(processing_times)\n",
    "        ms = compute_makespan(schedule, processing_times)\n",
    "        makespans.append(ms)\n",
    "\n",
    "    if not makespans:\n",
    "        return 1e9\n",
    "    return -float(np.mean(makespans))\n",
    "\n",
    "\n",
    "@funsearch.evolve\n",
    "def neh_heuristic(processing_times: np.ndarray) -> list[int]:\n",
    "    \"\"\"\n",
    "    An enhanced initial heuristic for the Permutation Flowshop Scheduling Problem (PFSP).\n",
    "\n",
    "    This heuristic combines:\n",
    "    - A weighted scoring for each job based on its total processing time and its maximum processing time.\n",
    "      The weight parameter alpha balances these two criteria.\n",
    "    - An iterative insertion procedure that builds an initial sequence.\n",
    "    - A subsequent local search using pairwise swap improvements to further reduce the makespan.\n",
    "\n",
    "    The resulting schedule (a list of job indices) is returned.\n",
    "    \"\"\"\n",
    "    num_jobs, num_machines = processing_times.shape\n",
    "    alpha = 0.7  # Weight parameter: can be tuned/evolved (alpha in [0, 1])\n",
    "\n",
    "    # Compute a weighted score for each job.\n",
    "    # Lower score indicates a job should be scheduled earlier.\n",
    "    job_scores = []\n",
    "    for job in range(num_jobs):\n",
    "        total_time = processing_times[job].sum()\n",
    "        max_time = processing_times[job].max()\n",
    "        score = alpha * total_time + (1 - alpha) * max_time\n",
    "        job_scores.append((job, score))\n",
    "\n",
    "    # Sort jobs by ascending score (best candidate first)\n",
    "    job_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Build an initial sequence using iterative insertion\n",
    "    sequence = [job_scores[0][0]]\n",
    "    for job, _ in job_scores[1:]:\n",
    "        best_sequence = None\n",
    "        best_makespan = float('inf')\n",
    "        # Try inserting the job in every possible position\n",
    "        for pos in range(len(sequence) + 1):\n",
    "            candidate_seq = sequence[:pos] + [job] + sequence[pos:]\n",
    "            ms = compute_makespan(candidate_seq, processing_times)\n",
    "            if ms < best_makespan:\n",
    "                best_makespan = ms\n",
    "                best_sequence = candidate_seq\n",
    "        sequence = best_sequence\n",
    "\n",
    "    # Local search: try pairwise swaps to further improve the sequence\n",
    "    improvement = True\n",
    "    while improvement:\n",
    "        improvement = False\n",
    "        current_makespan = compute_makespan(sequence, processing_times)\n",
    "        for i in range(num_jobs - 1):\n",
    "            for j in range(i + 1, num_jobs):\n",
    "                new_seq = sequence.copy()\n",
    "                new_seq[i], new_seq[j] = new_seq[j], new_seq[i]\n",
    "                new_makespan = compute_makespan(new_seq, processing_times)\n",
    "                if new_makespan < current_makespan:\n",
    "                    sequence = new_seq\n",
    "                    current_makespan = new_makespan\n",
    "                    improvement = True\n",
    "                    # Break out to restart the search after any improvement\n",
    "                    break\n",
    "            if improvement:\n",
    "                break\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "id": "2e2f875d128a693a",
    "ExecuteTime": {
     "end_time": "2025-04-06T07:37:48.163281Z",
     "start_time": "2025-04-06T07:37:48.160252Z"
    }
   },
   "id": "2e2f875d128a693a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Prepare a dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "391bfe61e1661e18"
   },
   "id": "391bfe61e1661e18"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def load_datasets(dataset_folder):\n",
    "    \"\"\"\n",
    "    Loads all datasets from the given folder, removing unnecessary job indices.\n",
    "\n",
    "    :param dataset_folder: Path to the dataset folder.\n",
    "    :return: Dictionary {dataset_name: job_matrix}\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "\n",
    "    for root, _, files in os.walk(dataset_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):  # Ensure only .txt files are read\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    first_line = f.readline().strip().split()\n",
    "                    n_jobs, n_machines = int(first_line[0]), int(first_line[1])  # Read job/machine count\n",
    "\n",
    "                    raw_data = np.loadtxt(f)\n",
    "                    jobs = raw_data[:, 1::2]  # Keep only even-indexed columns (machine times)\n",
    "\n",
    "                    if jobs.shape != (n_jobs, n_machines):\n",
    "                        print(f\" Warning: Mismatch in expected dimensions for {file_path}, skipping dataset.\")\n",
    "                        continue  # Skip invalid dataset\n",
    "\n",
    "                    datasets[file] = jobs  # Store dataset by filename\n",
    "\n",
    "    return datasets"
   ],
   "metadata": {
    "id": "fea85ccfc8c0ca6d",
    "ExecuteTime": {
     "end_time": "2025-04-06T07:37:49.579883Z",
     "start_time": "2025-04-06T07:37:49.575400Z"
    }
   },
   "id": "fea85ccfc8c0ca6d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Start FunSearch\n",
    "Please note that in jupyter notebook the following code will fail. This is because juypter does not support multiprocessing. Colab backend supports multiprocessing."
   ],
   "metadata": {
    "collapsed": false,
    "id": "cb66651fb2764ce9"
   },
   "id": "cb66651fb2764ce9"
  },
  {
   "cell_type": "code",
   "source": [
    "from implementation import funsearch\n",
    "from implementation import config\n",
    "\n",
    "# It should be noted that the if __name__ == '__main__' is required.\n",
    "# Because the inner code uses multiprocess evaluation.\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_path = \"/content/Funsearch_on_flowshop/data/\"\n",
    "    datasets = {}\n",
    "    for subfolder in [\"carlier\", \"heller\", \"reeves\"]:\n",
    "        datasets.update(load_datasets(os.path.join(data_path, subfolder)))\n",
    "\n",
    "    print(f\"Successfully loaded {len(datasets)} datasets.\")\n",
    "\n",
    "    instances = {\n",
    "        \"heller1.txt\": datasets[\"heller1.txt\"],\n",
    "        \"heller2.txt\": datasets[\"heller2.txt\"],\n",
    "        \"reeves1.txt\": datasets[\"reeves1.txt\"],\n",
    "    }\n",
    "    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)\n",
    "    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=30)\n",
    "    global_max_sample_num = 50  # if it is set to None, funsearch will execute an endless loop\n",
    "    funsearch.main(\n",
    "        specification=specification,\n",
    "        inputs=instances,\n",
    "        config=config,\n",
    "        max_sample_nums=global_max_sample_num,\n",
    "        class_config=class_config,\n",
    "        verbose=True,\n",
    "        log_dir='../logs/evaluator.log/'\n",
    "    )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e0ec0c796d09ca1",
    "outputId": "6a161b1d-9d4d-411d-9c6c-b9fcf10dc5a1",
    "ExecuteTime": {
     "end_time": "2025-04-06T07:39:11.016793Z",
     "start_time": "2025-04-06T07:38:37.981671Z"
    }
   },
   "id": "1e0ec0c796d09ca1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 31 datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/funsearch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/funsearch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Sandbox' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/funsearch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/funsearch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Sandbox' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Evaluated Function =================\n",
      "def neh_heuristic(processing_times: np.ndarray) -> list[int]:\n",
      "    \"\"\"\n",
      "    An enhanced initial heuristic for the Permutation Flowshop Scheduling Problem (PFSP).\n",
      "\n",
      "    This heuristic combines:\n",
      "    - A weighted scoring for each job based on its total processing time and its maximum processing time.\n",
      "      The weight parameter alpha balances these two criteria.\n",
      "    - An iterative insertion procedure that builds an initial sequence.\n",
      "    - A subsequent local search using pairwise swap improvements to further reduce the makespan.\n",
      "\n",
      "    The resulting schedule (a list of job indices) is returned.\n",
      "    \"\"\"\n",
      "    num_jobs, num_machines = processing_times.shape\n",
      "    alpha = 0.7  # Weight parameter: can be tuned/evolved (alpha in [0, 1])\n",
      "\n",
      "    # Compute a weighted score for each job.\n",
      "    # Lower score indicates a job should be scheduled earlier.\n",
      "    job_scores = []\n",
      "    for job in range(num_jobs):\n",
      "        total_time = processing_times[job].sum()\n",
      "        max_time = processing_times[job].max()\n",
      "        score = alpha * total_time + (1 - alpha) * max_time\n",
      "        job_scores.append((job, score))\n",
      "\n",
      "    # Sort jobs by ascending score (best candidate first)\n",
      "    job_scores.sort(key=lambda x: x[1])\n",
      "\n",
      "    # Build an initial sequence using iterative insertion\n",
      "    sequence = [job_scores[0][0]]\n",
      "    for job, _ in job_scores[1:]:\n",
      "        best_sequence = None\n",
      "        best_makespan = float('inf')\n",
      "        # Try inserting the job in every possible position\n",
      "        for pos in range(len(sequence) + 1):\n",
      "            candidate_seq = sequence[:pos] + [job] + sequence[pos:]\n",
      "            ms = compute_makespan(candidate_seq, processing_times)\n",
      "            if ms < best_makespan:\n",
      "                best_makespan = ms\n",
      "                best_sequence = candidate_seq\n",
      "        sequence = best_sequence\n",
      "\n",
      "    # Local search: try pairwise swaps to further improve the sequence\n",
      "    improvement = True\n",
      "    while improvement:\n",
      "        improvement = False\n",
      "        current_makespan = compute_makespan(sequence, processing_times)\n",
      "        for i in range(num_jobs - 1):\n",
      "            for j in range(i + 1, num_jobs):\n",
      "                new_seq = sequence.copy()\n",
      "                new_seq[i], new_seq[j] = new_seq[j], new_seq[i]\n",
      "                new_makespan = compute_makespan(new_seq, processing_times)\n",
      "                if new_makespan < current_makespan:\n",
      "                    sequence = new_seq\n",
      "                    current_makespan = new_makespan\n",
      "                    improvement = True\n",
      "                    # Break out to restart the search after any improvement\n",
      "                    break\n",
      "            if improvement:\n",
      "                break\n",
      "\n",
      "    return sequence\n",
      "------------------------------------------------------\n",
      "Score        : None\n",
      "Sample time  : None\n",
      "Evaluate time: 0.10454511642456055\n",
      "Sample orders: None\n",
      "======================================================\n",
      "\n",
      "\n",
      "[Generation 1] Current Best Score: -99999999\n",
      "Best Program Sample Order: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/funsearch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/funsearch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'Sandbox' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     21\u001B[39m config = config.Config(samples_per_prompt=\u001B[32m4\u001B[39m, evaluate_timeout_seconds=\u001B[32m30\u001B[39m)\n\u001B[32m     22\u001B[39m global_max_sample_num = \u001B[32m50\u001B[39m  \u001B[38;5;66;03m# if it is set to None, funsearch will execute an endless loop\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43mfunsearch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mspecification\u001B[49m\u001B[43m=\u001B[49m\u001B[43mspecification\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minstances\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_sample_nums\u001B[49m\u001B[43m=\u001B[49m\u001B[43mglobal_max_sample_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mclass_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclass_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m../logs/evaluator.log/\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m     31\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/CityU/SemesterB/Artificial Intelligence/project/Funsearch_on_flowshop/implementation/funsearch.py:102\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m(specification, inputs, config, max_sample_nums, class_config, **kwargs)\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;66;03m# This loop can be executed in parallel on remote sampler machines. As each\u001B[39;00m\n\u001B[32m     99\u001B[39m \u001B[38;5;66;03m# sampler enters an infinite loop, without parallelization only the first\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[38;5;66;03m# sampler will do any work.\u001B[39;00m\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m samplers:\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m     \u001B[43ms\u001B[49m\u001B[43m.\u001B[49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprofiler\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprofiler\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/CityU/SemesterB/Artificial Intelligence/project/Funsearch_on_flowshop/implementation/sampler.py:116\u001B[39m, in \u001B[36mSampler.sample\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    107\u001B[39m         chosen_evaluator.analyse(\n\u001B[32m    108\u001B[39m             sample,\n\u001B[32m    109\u001B[39m             prompt.island_id,\n\u001B[32m   (...)\u001B[39m\u001B[32m    113\u001B[39m             sample_time=sample_time\n\u001B[32m    114\u001B[39m         )\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
