{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakura-RaidenMEI/Funsearch_on_flowshop/blob/main/flowshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run FunSearch on Bin Packing\n",
        "Five steps:\n",
        "1. Implement 'LLM' interface.\n",
        "2. Implement a 'SandBox' interface.\n",
        "3. Prepare a 'specification'.\n",
        "4. Prepare a dataset.\n",
        "5. Start FunSearch."
      ],
      "metadata": {
        "collapsed": false,
        "id": "58ba1915fced4e72"
      },
      "id": "58ba1915fced4e72"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation: download the project file from github. And update system path."
      ],
      "metadata": {
        "collapsed": false,
        "id": "6a2d02b8e9c3ba67"
      },
      "id": "6a2d02b8e9c3ba67"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'funsearch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Sakura-RaidenMEI/Funsearch_on_flowshop.git\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/Funsearch_on_flowshop/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22453e8153e0934c",
        "outputId": "541755c5-1f9e-4dcf-9507-1ff64d44cba8"
      },
      "id": "22453e8153e0934c"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZL3i4JLo2oDW"
      },
      "id": "ZL3i4JLo2oDW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Implement LLM interface\n",
        "Set the API's IP address according to your API provider (See line 65 in the following code).\n",
        "```python\n",
        "conn = http.client.HTTPSConnection(\"api.chatanywhere.com.cn\")\n",
        "```\n",
        "You should prepare a 'key' for the LLM API. And fill them in the header (See line 76-80 in the following code).\n",
        "```python\n",
        "headers = {\n",
        "    'Authorization': 'Bearer [put your key here, the key may start with \"sk-...\"]',\n",
        "    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "collapsed": false,
        "id": "fe47175708cc0a93"
      },
      "id": "fe47175708cc0a93"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import multiprocessing\n",
        "from typing import Collection, Any\n",
        "import http.client\n",
        "from implementation import sampler\n",
        "from openai import OpenAI\n",
        "\n",
        "def _trim_preface_of_body(sample: str) -> str:\n",
        "    \"\"\"Trim the redundant descriptions/symbols/'def' declaration before the function body.\n",
        "    Please see my comments in sampler.LLM (in sampler.py).\n",
        "    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.\n",
        "\n",
        "    -Example sample (function & description generated by LLM):\n",
        "    -------------------------------------\n",
        "    This is the optimized function ...\n",
        "    def priority_v2(...) -> ...:\n",
        "        return ...\n",
        "    This function aims to ...\n",
        "    -------------------------------------\n",
        "    -This function removes the description above the function's signature, and the function's signature.\n",
        "    -The indent of the code is preserved.\n",
        "    -Return of this function:\n",
        "    -------------------------------------\n",
        "        return ...\n",
        "    This function aims to ...\n",
        "    -------------------------------------\n",
        "    \"\"\"\n",
        "    lines = sample.splitlines()\n",
        "    func_body_lineno = 0\n",
        "    find_def_declaration = False\n",
        "    for lineno, line in enumerate(lines):\n",
        "        # find the first 'def' statement in the given code\n",
        "        if line[:3] == 'def':\n",
        "            func_body_lineno = lineno\n",
        "            find_def_declaration = True\n",
        "            break\n",
        "    if find_def_declaration:\n",
        "        code = ''\n",
        "        for line in lines[func_body_lineno + 1:]:\n",
        "            code += line + '\\n'\n",
        "        return code\n",
        "    return sample\n",
        "\n",
        "\n",
        "class LLMAPI(sampler.LLM):\n",
        "    \"\"\"Language model that predicts continuation of provided source code.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, samples_per_prompt: int, trim=True):\n",
        "        super().__init__(samples_per_prompt)\n",
        "        additional_prompt = ('Complete a different and more complex Python function. '\n",
        "                             'Be creative and you can insert multiple if-else and for-loop in the code logic.'\n",
        "                             'Only output the Python code, no descriptions.')\n",
        "\n",
        "        self._additional_prompt = additional_prompt\n",
        "        self._trim = trim\n",
        "\n",
        "    def draw_samples(self, prompt: str) -> Collection[str]:\n",
        "        \"\"\"Returns multiple predicted continuations of `prompt`.\"\"\"\n",
        "        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]\n",
        "\n",
        "    def _draw_sample(self, content: str) -> str:\n",
        "        prompt = '\\n'.join([content, self._additional_prompt])\n",
        "        message = [{'role': 'user', 'content': prompt}]\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                conn = http.client.HTTPSConnection(\"api.bltcy.ai\")\n",
        "                payload = json.dumps({\n",
        "                    \"max_tokens\": 1024,\n",
        "                    \"model\": \"gpt-3.5-turbo\",\n",
        "                    \"messages\": [\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": prompt\n",
        "                        }\n",
        "                    ]\n",
        "                })\n",
        "                headers = {\n",
        "                    'Authorization': 'Bearer sk-OmRJlpj2aI4A3GLvA4Bd841fCfB04b3e9eF6D0D9984f1719',\n",
        "                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
        "                    'Content-Type': 'application/json'\n",
        "                }\n",
        "                conn.request(\"POST\", \"/v1/chat/completions\", payload, headers)\n",
        "                res = conn.getresponse()\n",
        "                data = res.read().decode(\"utf-8\")\n",
        "                data = json.loads(data)\n",
        "                response = data['choices'][0]['message']['content']\n",
        "                if self._trim:\n",
        "                    response = _trim_preface_of_body(response)\n",
        "                return response\n",
        "            except Exception:\n",
        "                time.sleep(2)\n",
        "                continue"
      ],
      "metadata": {
        "id": "1999e45c9a568b08"
      },
      "id": "1999e45c9a568b08"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implement a 'SandBox' interface"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d27817cdec2cedfc"
      },
      "id": "d27817cdec2cedfc"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "from implementation import evaluator\n",
        "from implementation import evaluator_accelerate\n",
        "\n",
        "\n",
        "class Sandbox(evaluator.Sandbox):\n",
        "    \"\"\"Sandbox for executing generated code. Implemented by RZ.\n",
        "\n",
        "    RZ: Sandbox returns the 'score' of the program and:\n",
        "    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).\n",
        "    2) stops the execution of the code in time (avoid endless loop).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False, numba_accelerate=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            verbose         : Print evaluate information.\n",
        "            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions\n",
        "                              support numba acceleration, such as np.piecewise().\n",
        "        \"\"\"\n",
        "        self._verbose = verbose\n",
        "        self._numba_accelerate = numba_accelerate\n",
        "\n",
        "    def run(\n",
        "            self,\n",
        "            program: str,\n",
        "            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')\n",
        "            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.\n",
        "            inputs: Any,  # refers to the dataset\n",
        "            test_input: str,  # refers to the current instance\n",
        "            timeout_seconds: int,\n",
        "            **kwargs  # RZ: add this\n",
        "    ) -> tuple[Any, bool]:\n",
        "        \"\"\"Returns `function_to_run(test_input)` and whether execution succeeded.\n",
        "\n",
        "        RZ: If the generated code (generated by LLM) is executed successfully,\n",
        "        the output of this function is the score of a given program.\n",
        "        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.\n",
        "        \"\"\"\n",
        "        dataset = inputs[test_input]\n",
        "        try:\n",
        "            result_queue = multiprocessing.Queue()\n",
        "            process = multiprocessing.Process(\n",
        "                target=self._compile_and_run_function,\n",
        "                args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)\n",
        "            )\n",
        "            process.start()\n",
        "            process.join(timeout=timeout_seconds)\n",
        "            if process.is_alive():\n",
        "                # if the process is not finished in time, we consider the program illegal\n",
        "                process.terminate()\n",
        "                process.join()\n",
        "                results = None, False\n",
        "            else:\n",
        "                if not result_queue.empty():\n",
        "                    results = result_queue.get_nowait()\n",
        "                else:\n",
        "                    results = None, False\n",
        "\n",
        "            return results\n",
        "        except:\n",
        "            return None, False\n",
        "\n",
        "    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,\n",
        "                                  result_queue):\n",
        "        try:\n",
        "            # optimize the code (decorate function_to_run with @numba.jit())\n",
        "            if numba_accelerate:\n",
        "                program = evaluator_accelerate.add_numba_decorator(\n",
        "                    program=program,\n",
        "                    function_to_evolve=function_to_evolve\n",
        "                )\n",
        "            # compile the program, and maps the global func/var/class name to its address\n",
        "            all_globals_namespace = {}\n",
        "            # execute the program, map func/var/class to global namespace\n",
        "            exec(program, all_globals_namespace)\n",
        "            # get the pointer of 'function_to_run'\n",
        "            function_to_run = all_globals_namespace[function_to_run]\n",
        "            # return the execution results\n",
        "            results = function_to_run(dataset)\n",
        "            # the results must be int or float\n",
        "            if not isinstance(results, (int, float)):\n",
        "                result_queue.put((None, False))\n",
        "                return\n",
        "            result_queue.put((results, True))\n",
        "        except Exception:\n",
        "            # if raise any exception, we assume the execution failed\n",
        "            result_queue.put((None, False))"
      ],
      "metadata": {
        "id": "3e3d88a87535b6b2"
      },
      "id": "3e3d88a87535b6b2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare a 'specification'"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ec3a05827354f9ae"
      },
      "id": "ec3a05827354f9ae"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "specification = r'''\n",
        "import numpy as np\n",
        "from implementation import funsearch\n",
        "\n",
        "def compute_makespan(schedule: list[int], processing_times: np.ndarray) -> int:\n",
        "    \"\"\"\n",
        "    Compute the makespan (total completion time) for a given job schedule in a PFSP.\n",
        "    - schedule: list of job indices in the order they are processed.\n",
        "    - processing_times: 2D numpy array of shape (num_jobs, num_machines) with processing times for each job on each machine.\n",
        "    Returns the makespan (int) for the given order.\n",
        "    \"\"\"\n",
        "    num_jobs = len(schedule)\n",
        "    num_machines = processing_times.shape[1]\n",
        "    if num_jobs == 0:\n",
        "        return 0\n",
        "\n",
        "    # Initialize a matrix to keep track of completion times.\n",
        "    completion_times = np.zeros((num_jobs, num_machines), dtype=int)\n",
        "\n",
        "    # Set completion times for the first job in the sequence.\n",
        "    first_job = schedule[0]\n",
        "    completion_times[0, 0] = processing_times[first_job, 0]\n",
        "    for m in range(1, num_machines):\n",
        "        completion_times[0, m] = completion_times[0, m-1] + processing_times[first_job, m]\n",
        "\n",
        "    # Calculate completion times for the rest of the jobs in the sequence.\n",
        "    for i in range(1, num_jobs):\n",
        "        job = schedule[i]\n",
        "        # Machine 0: add processing time of this job to completion time of previous job on machine 0.\n",
        "        completion_times[i, 0] = completion_times[i-1, 0] + processing_times[job, 0]\n",
        "        # Other machines:\n",
        "        for m in range(1, num_machines):\n",
        "            # The job can start on machine m only after it finishes on machine m-1 AND the previous job finishes on machine m.\n",
        "            completion_times[i, m] = max(completion_times[i, m-1], completion_times[i-1, m]) + processing_times[job, m]\n",
        "\n",
        "    # The makespan is the completion time of the last job on the last machine.\n",
        "    makespan = completion_times[num_jobs-1, num_machines-1]\n",
        "    return int(makespan)\n",
        "\n",
        "@funsearch.evolve\n",
        "def neh_heuristic(processing_times: np.ndarray) -> list[int]:\n",
        "    \"\"\"\n",
        "    NEH heuristic for PFSP that generates a job permutation (schedule) given the processing times matrix.\n",
        "    Decorated with @funsearch.evolve so FunSearch can evolve this heuristic for better ordering.\n",
        "    \"\"\"\n",
        "    num_jobs = processing_times.shape[0]\n",
        "    # Step 1: Calculate total processing time of each job across all machines.\n",
        "    total_times = [(job, processing_times[job].sum()) for job in range(num_jobs)]\n",
        "    # Step 2: Sort jobs in descending order of total processing time.\n",
        "    total_times.sort(key=lambda x: x[1], reverse=True)\n",
        "    # Step 3: Construct an initial sequence with the first (heaviest) job.\n",
        "    sequence = [total_times[0][0]]\n",
        "    # Step 4: Iteratively insert each of the remaining jobs into the sequence at the position that minimizes makespan.\n",
        "    for job, _ in total_times[1:]:\n",
        "        best_position = None\n",
        "        best_makespan = float('inf')\n",
        "        # Try inserting the job at every possible position in the current sequence.\n",
        "        for pos in range(len(sequence) + 1):\n",
        "            candidate_seq = sequence[:pos] + [job] + sequence[pos:]\n",
        "            # Compute makespan for this candidate sequence.\n",
        "            ms = compute_makespan(candidate_seq, processing_times)\n",
        "            if ms < best_makespan:\n",
        "                best_makespan = ms\n",
        "                best_position = candidate_seq\n",
        "        # Fix the sequence to the best found ordering for this iteration.\n",
        "        sequence = best_position\n",
        "    return sequence\n",
        "\n",
        "@funsearch.run\n",
        "def evaluate(instances: dict) -> float:\n",
        "    makespans = []\n",
        "    for name in instances:\n",
        "        processing_times = instances[name][\"processing_times\"]\n",
        "        schedule = neh_heuristic(processing_times)\n",
        "        ms = compute_makespan(schedule, processing_times)\n",
        "        makespans.append(ms)\n",
        "    return float(np.mean(makespans))\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "2e2f875d128a693a"
      },
      "id": "2e2f875d128a693a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare a dataset"
      ],
      "metadata": {
        "collapsed": false,
        "id": "391bfe61e1661e18"
      },
      "id": "391bfe61e1661e18"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "import bin_packing_utils\n",
        "\n",
        "bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}"
      ],
      "metadata": {
        "id": "fea85ccfc8c0ca6d"
      },
      "id": "fea85ccfc8c0ca6d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Start FunSearch\n",
        "Please note that in jupyter notebook the following code will fail. This is because juypter does not support multiprocessing. Colab backend supports multiprocessing."
      ],
      "metadata": {
        "collapsed": false,
        "id": "cb66651fb2764ce9"
      },
      "id": "cb66651fb2764ce9"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Best score of island 0 increased to -500.0\n",
            "INFO:absl:Best score of island 1 increased to -500.0\n",
            "INFO:absl:Best score of island 2 increased to -500.0\n",
            "INFO:absl:Best score of island 3 increased to -500.0\n",
            "INFO:absl:Best score of island 4 increased to -500.0\n",
            "INFO:absl:Best score of island 5 increased to -500.0\n",
            "INFO:absl:Best score of island 6 increased to -500.0\n",
            "INFO:absl:Best score of island 7 increased to -500.0\n",
            "INFO:absl:Best score of island 8 increased to -500.0\n",
            "INFO:absl:Best score of island 9 increased to -500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    log_ratios = np.log(ratios)\n",
            "    priorities = -log_ratios\n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -500.0\n",
            "Sample time  : None\n",
            "Evaluate time: 3.469514846801758\n",
            "Sample orders: None\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:absl:Best score of island 1 increased to -212.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if item > bins[i]:\n",
            "            priorities[i] = -1\n",
            "        else:\n",
            "            priorities[i] = item / bins[i]\n",
            "    \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -212.4\n",
            "Sample time  : 1.064006745815277\n",
            "Evaluate time: 1.4711005687713623\n",
            "Sample orders: 2\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    for i in range(len(bins)):\n",
            "        if item > bins[i]:\n",
            "            priorities[i] = -1\n",
            "        else:\n",
            "            priorities[i] = item / bins[i]\n",
            "    \n",
            "    max_priority = np.max(priorities)\n",
            "    if max_priority > 0:\n",
            "        priorities = priorities / max_priority\n",
            "    \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : None\n",
            "Sample time  : 1.064006745815277\n",
            "Evaluate time: 2.0159928798675537\n",
            "Sample orders: 3\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    \"\"\"Improved version of `priority_v0`.\"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    for i in range(len(bins)):\n",
            "        if bins[i] >= item:\n",
            "            priorities[i] = 1\n",
            "        else:\n",
            "            priorities[i] = item / bins[i]\n",
            "    \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -212.75\n",
            "Sample time  : 1.064006745815277\n",
            "Evaluate time: 1.0401902198791504\n",
            "Sample orders: 4\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    for i, bin_capacity in enumerate(bins):\n",
            "        if item <= bin_capacity:\n",
            "            priorities[i] = 1 / (bin_capacity - item + 1)\n",
            "        else:\n",
            "            priorities[i] = 0.5 * item / bin_capacity\n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -212.4\n",
            "Sample time  : 1.064006745815277\n",
            "Evaluate time: 1.0582342147827148\n",
            "Sample orders: 5\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:absl:Best score of island 5 increased to -311.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    priorities = []\n",
            "    \n",
            "    for ratio in ratios:\n",
            "        if ratio < 0.2:\n",
            "            priorities.append(1.0)\n",
            "        elif 0.2 <= ratio < 0.5:\n",
            "            priorities.append(0.8)\n",
            "        elif 0.5 <= ratio < 0.8:\n",
            "            priorities.append(0.5)\n",
            "        else:\n",
            "            priorities.append(0.2)\n",
            "    \n",
            "    return np.array(priorities)\n",
            "------------------------------------------------------\n",
            "Score        : -311.7\n",
            "Sample time  : 1.1171817779541016\n",
            "Evaluate time: 1.1975593566894531\n",
            "Sample orders: 6\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Best score of island 5 increased to -212.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    for i in range(len(bins)):\n",
            "        if item <= bins[i]:\n",
            "            priorities[i] = 1.0\n",
            "        else:\n",
            "            remaining_space = bins[i]\n",
            "            remaining_item = item\n",
            "            while remaining_space > 0 and remaining_item > 0:\n",
            "                ratio = remaining_item / remaining_space\n",
            "                if ratio > 1.0:\n",
            "                    priorities[i] += 1.0\n",
            "                    remaining_space = 0\n",
            "                else:\n",
            "                    priorities[i] += ratio\n",
            "                    remaining_space = 0\n",
            "                remaining_item -= remaining_space\n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -212.75\n",
            "Sample time  : 1.1171817779541016\n",
            "Evaluate time: 1.0660803318023682\n",
            "Sample orders: 7\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if bins[i] >= item:\n",
            "            priorities[i] = 1.0\n",
            "        else:\n",
            "            remaining_space = bins[i] - item\n",
            "            utilization = item / bins[i]\n",
            "            priority = utilization * np.exp(remaining_space)\n",
            "            priorities[i] = priority\n",
            "    \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -212.75\n",
            "Sample time  : 1.1171817779541016\n",
            "Evaluate time: 1.3143877983093262\n",
            "Sample orders: 8\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Best score of island 5 increased to -212.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = []\n",
            "    \n",
            "    for bin_capacity in bins:\n",
            "        if item <= bin_capacity:\n",
            "            priority = 1 / (bin_capacity - item + 1)\n",
            "        else:\n",
            "            priority = 0\n",
            "        priorities.append(priority)\n",
            "    \n",
            "    return np.array(priorities)\n",
            "------------------------------------------------------\n",
            "Score        : -212.0\n",
            "Sample time  : 1.1171817779541016\n",
            "Evaluate time: 1.2855141162872314\n",
            "Sample orders: 9\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:absl:Best score of island 8 increased to -212.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        remaining_space = bins[i] - item\n",
            "        \n",
            "        if remaining_space < 0:\n",
            "            priorities[i] = -np.inf\n",
            "        elif remaining_space == 0:\n",
            "            priorities[i] = np.inf\n",
            "        else:\n",
            "            priorities[i] = 1 / remaining_space\n",
            "    \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -212.25\n",
            "Sample time  : 1.040146827697754\n",
            "Evaluate time: 1.108713150024414\n",
            "Sample orders: 10\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if item <= 0:\n",
            "            priorities[i] = float('inf')\n",
            "        elif item > bins[i]:\n",
            "            priorities[i] = -np.inf\n",
            "        else:\n",
            "            priorities[i] = bins[i] - item\n",
            "\n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -500.0\n",
            "Sample time  : 1.040146827697754\n",
            "Evaluate time: 1.075763463973999\n",
            "Sample orders: 11\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    if item > np.sum(bins):\n",
            "        raise ValueError(\"Item size is larger than total bin capacity\")\n",
            "\n",
            "    priorities = np.zeros_like(bins)\n",
            "    remaining_space = bins.copy()\n",
            "\n",
            "    for i in range(len(bins)):\n",
            "        if item <= remaining_space[i]:\n",
            "            priorities[i] = item / bins[i]\n",
            "            break\n",
            "        else:\n",
            "            priorities[i] = 1.0\n",
            "            item -= remaining_space[i]\n",
            "            remaining_space[i] = 0\n",
            "\n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -212.75\n",
            "Sample time  : 1.040146827697754\n",
            "Evaluate time: 1.2407588958740234\n",
            "Sample orders: 12\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = []\n",
            "    for bin_capacity in bins:\n",
            "        if item <= bin_capacity:\n",
            "            priority = 1 / item\n",
            "        else:\n",
            "            priority = 0\n",
            "        priorities.append(priority)\n",
            "    return np.array(priorities)\n",
            "------------------------------------------------------\n",
            "Score        : -212.75\n",
            "Sample time  : 1.040146827697754\n",
            "Evaluate time: 0.8928437232971191\n",
            "Sample orders: 13\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from implementation import funsearch\n",
        "from implementation import config\n",
        "\n",
        "# It should be noted that the if __name__ == '__main__' is required.\n",
        "# Because the inner code uses multiprocess evaluation.\n",
        "if __name__ == '__main__':\n",
        "    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)\n",
        "    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=30)\n",
        "    global_max_sample_num = 10  # if it is set to None, funsearch will execute an endless loop\n",
        "    funsearch.main(\n",
        "        specification=specification,\n",
        "        inputs=bin_packing_or3,\n",
        "        config=config,\n",
        "        max_sample_nums=global_max_sample_num,\n",
        "        class_config=class_config,\n",
        "        log_dir='../logs/funsearch_llm_api'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e0ec0c796d09ca1",
        "outputId": "5380c5dd-9b81-40b3-bc9b-152e63997acc"
      },
      "id": "1e0ec0c796d09ca1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
